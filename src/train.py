"""
Script d'entraÃ®nement avec tracking MLflow
Permet de comparer diffÃ©rents modÃ¨les et hyperparamÃ¨tres
"""
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import sys
import os

# Ajouter le dossier src au path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.preprocess import prepare_data

def evaluate_model(model, X_test, y_test):
    """Ã‰value un modÃ¨le et retourne les mÃ©triques"""
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, predictions)
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    }

def train_model(model_type='random_forest', **kwargs):
    """
    EntraÃ®ne un modÃ¨le avec MLflow tracking
    
    Args:
        model_type: 'random_forest', 'gradient_boosting', ou 'ridge'
        **kwargs: HyperparamÃ¨tres du modÃ¨le
    """
    # 1ï¸âƒ£ PrÃ©parer les donnÃ©es
    print("ðŸ“Š PrÃ©paration des donnÃ©es...")
    X_train, X_test, y_train, y_test, preprocessor = prepare_data()
    
    # 2ï¸âƒ£ Configurer MLflow
    mlflow.set_experiment("car_price_prediction")
    
    with mlflow.start_run(run_name=f"{model_type}_experiment"):
        
        # 3ï¸âƒ£ CrÃ©er le modÃ¨le selon le type
        if model_type == 'random_forest':
            model = RandomForestRegressor(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', None),
                random_state=42
            )
        elif model_type == 'gradient_boosting':
            model = GradientBoostingRegressor(
                n_estimators=kwargs.get('n_estimators', 100),
                learning_rate=kwargs.get('learning_rate', 0.1),
                max_depth=kwargs.get('max_depth', 3),
                random_state=42
            )
        elif model_type == 'ridge':
            model = Ridge(
                alpha=kwargs.get('alpha', 1.0),
                random_state=42
            )
        else:
            raise ValueError(f"Type de modÃ¨le inconnu : {model_type}")
        
        # 4ï¸âƒ£ Logger les paramÃ¨tres
        mlflow.log_param("model_type", model_type)
        for key, value in kwargs.items():
            mlflow.log_param(key, value)
        
        # 5ï¸âƒ£ EntraÃ®ner le modÃ¨le
        print(f"ðŸ‹ï¸ EntraÃ®nement du modÃ¨le {model_type}...")
        model.fit(X_train, y_train)
        
        # 6ï¸âƒ£ Ã‰valuer le modÃ¨le
        metrics = evaluate_model(model, X_test, y_test)
        print(f"\nðŸ“ˆ RÃ©sultats :")
        print(f"  MAE:  {metrics['mae']:.2f} â‚¬")
        print(f"  RMSE: {metrics['rmse']:.2f} â‚¬")
        print(f"  RÂ²:   {metrics['r2']:.4f}")
        
        # 7ï¸âƒ£ Logger les mÃ©triques
        mlflow.log_metric("mae", metrics['mae'])
        mlflow.log_metric("rmse", metrics['rmse'])
        mlflow.log_metric("r2", metrics['r2'])
        
        # 8ï¸âƒ£ Sauvegarder le modÃ¨le dans MLflow
        mlflow.sklearn.log_model(model, "model")
        
        # 9ï¸âƒ£ Sauvegarder aussi localement si c'est le meilleur
        import joblib
        os.makedirs('models', exist_ok=True)
        joblib.dump(model, 'models/production_model.pkl')
        preprocessor.save('models/preprocessor.pkl')
        
        print(f"\nâœ… ModÃ¨le sauvegardÃ© !")
        print(f"ðŸ“‚ MLflow UI : mlflow ui --port 5000")
        
        return model, metrics

def compare_models():
    """Compare plusieurs modÃ¨les et affiche les rÃ©sultats"""
    print("ðŸ”¬ Comparaison de plusieurs modÃ¨les...\n")
    
    experiments = [
        {'model_type': 'random_forest', 'n_estimators': 50, 'max_depth': 10},
        {'model_type': 'random_forest', 'n_estimators': 100, 'max_depth': 15},
        {'model_type': 'gradient_boosting', 'n_estimators': 100, 'learning_rate': 0.1},
        {'model_type': 'ridge', 'alpha': 1.0}
    ]
    
    results = []
    for exp in experiments:
        print(f"\n{'='*60}")
        print(f"Test : {exp}")
        print('='*60)
        model, metrics = train_model(**exp)
        results.append({**exp, **metrics})
    
    # Afficher le rÃ©sumÃ©
    print("\n" + "="*80)
    print("ðŸ“Š RÃ‰SUMÃ‰ DES EXPÃ‰RIENCES")
    print("="*80)
    results_sorted = sorted(results, key=lambda x: x['rmse'])
    for i, r in enumerate(results_sorted, 1):
        print(f"\n{i}. {r['model_type']} - RMSE: {r['rmse']:.2f} â‚¬ - RÂ²: {r['r2']:.4f}")
        print(f"   ParamÃ¨tres: {', '.join([f'{k}={v}' for k,v in r.items() if k not in ['mae','rmse','r2','model_type']])}")
    
    print("\nðŸ† Meilleur modÃ¨le sauvegardÃ© dans models/production_model.pkl")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'compare':
        # Mode comparaison : python src/train.py compare
        compare_models()
    else:
        # Mode simple : python src/train.py
        train_model(model_type='random_forest', n_estimators=100, max_depth=15)